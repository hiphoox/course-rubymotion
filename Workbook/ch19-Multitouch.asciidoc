Chapter 19 - Multitouch
=======================

One of the best features of the iPhone or iPad is the touch screen, in this chapter we will use it to make a app for drawing on the screen


Kanji Master
------------

One of the most difficult languages to learn is Japanese because they use one symbol (Kanji) to express full concepts. This is different than most of the languages, because they use words (Multiple symbols: letters) for the same purpose. 

In this exercise we will create a app that will teach the user how to draw a Kanji, allowing him to paint the shape with his fingers on the screen.

.Kanji
image::resources/ch19-Multitouch/ch19_Kanji.png[Kanji]


Preparing the Canvas
~~~~~~~~~~~~~~~~~~~~

Lets start creating a new project called **KanjiMaster**:

[source, sh]
------------
$ motion create KanjiMaster
------------

Please copy the resources from **DEFINE DEPLOYMENT PATH** into the project resources folder, after that create the following file:

[source, sh]
------------
$ cd KanjiMaster/app

$ mkdir views

$ touch kanji_view.rb

$ open kanji_view.rb
------------

Now add the following to our **kanji_view.rb** file:

[source, ruby]
--------------
class KanjiView < UIView

  def initWithFrame(frame)

    if super

      # In order to does not hide the background image on
      # the controllers view, we need to set the background color on
      # this view as transparent
      self.backgroundColor = UIColor.clearColor

      # We need to set multipleTouchEnabled to true if we want to handle
      # more than one finger touching the screen
      self.multipleTouchEnabled = true

      # Instance an Array for storing the user touches
      @touch_points = NSMutableArray.alloc.init
    end

    self

  end

end
--------------

Lets create a controller for our new view with the name **KanjiViewController**:

[source, sh]
------------
$ cd ..

$ mkdir controllers

$ cd controllers

$ touch kanji_view_controller.rb

$ open kanji_view_controller.rb
------------

Insert the following into the **kanji_view_controller.rb** file:

[source, ruby]
------------
class KanjiViewController < UIViewController

  def loadView

    self.view = UIView.alloc.initWithFrame(UIScreen.mainScreen.bounds)

    # Create a new UIImageView to draw our background image
    background_image_view = UIImageView.alloc.initWithFrame(self.view.bounds)

    # Set our background image into the UIImageView
    background_image_view.image = UIImage.imageNamed("bgKanji.jpg")

    # Add the UIImageView instance into the view
    self.view.addSubview(background_image_view)

    # Load our custom UIView (KanjiView)
    kanjiView = KanjiView.alloc.initWithFrame(self.view.bounds)

    # Add it to our view controller's view 
    self.view.addSubview(kanjiView)
  end

end
------------

Finally lets open our **app_delegate.rb** and add the following:

[source, sh]
------------
$ cd ..

$ open app_delegate.rb
------------

[source, ruby]
--------------
class AppDelegate

  def application(application, didFinishLaunchingWithOptions:launchOptions)
    
    #Create an instance of Kanji View Controller
    kanji_view_controller = KanjiViewController.alloc.init

    @window = UIWindow.alloc.initWithFrame(UIScreen.mainScreen.bounds)

    #Every window has a root view controller from which it will present its view
    @window.rootViewController = kanji_view_controller
    @window.makeKeyAndVisible

    true
  end

end
--------------

If we run the app, you should see the following:

[source, sh]
------------
$ rake
------------

.Initial Application
image::resources/ch19-Multitouch/ch19_InitialApplication.png[Initial Application]


The Responder Chain
~~~~~~~~~~~~~~~~~~~
If you look closely on the project that we just create, we have to implement a custom view called **KanjiView**. The purpose of this is that we can catch the touch events on it. But why on a UIView? 

When the user touch the screen a new event is created and send as a message to the first responder (The most foremost view in the hierarchy), if it cannot handle the message, is forwarded to the "next responder" and so on. This linked series is called **Responder Chain**

In iOS (not as same as Mac) both UIViews and UIViewControllers are part of the **Responder Chain**, but typically just the Views can handle the touch events, this happens because by default the UIViewControllers can not become fist responders

According to the above we need a object that can become first responder and be in the responder chain so it can handle the touch events, this object generally is a UIView

IMPORTANT: If you need by any reason to handle the touch events on your controller, you can override the method **canBecomeFirstResponder** to allow it to become first responder 


Handling the Touches
~~~~~~~~~~~~~~~~~~~~
Now that we understand the Responder Chain, How to handle the touches? The response is implementing the following methods on our view:

[source, ruby]
--------------
# This method is called when the finger (or fingers) touch the screen for the first time
def touchesBegan(touches, withEvent: event)

# This method is called when the finger (or fingers) are moving without leaving the screen
def touchesMoved(touches, withEvent: event)

# This method is called when the finger (or fingers) leave the screen
def touchesEnded(touches, withEvent:event)
--------------

For testing proposes lets implement it on our **KanjiView** the following way:

[source, ruby]
--------------
# This method is called when the finger (or fingers) 
# touch the screen for the first time
def touchesBegan(touches, withEvent: event)

  #  Touches is an set of UITouch, each of them
  #  represent a diferent finger on the screen
  touches.allObjects.each_with_index { | touch, index |

    # We need to ask the touch for his location according
    # to the current view
    pointInView = touch.locationInView(self)

    NSLog("Touch %@ starting on %@", index + 1, NSStringFromCGPoint(pointInView))
  }

end

# This method is called when the finger (or fingers) 
# are moving without leaving the screen
def touchesMoved(touches, withEvent: event)

  #  Touches is an set of UITouch, each of them
  #  represent a diferent finger on the screen
  touches.allObjects.each_with_index { | touch, index |

    # We need to ask the touch for his location according
    # to the current view
    pointInView = touch.locationInView(self)

    NSLog("Touch %@ moving to %@", index + 1, NSStringFromCGPoint(pointInView))   
  }

end

# This method is called when the finger (or fingers) 
# leave the screen
def touchesEnded(touches, withEvent:event)

  #  Touches is an set of UITouch, each of them
  #  represent a diferent finger on the screen
  touches.allObjects.each_with_index { | touch, index |

    # We need to ask the touch for his location according
    # to the current view
    pointInView = touch.locationInView(self)

    NSLog("Touch %@ ended at %@", index + 1, NSStringFromCGPoint(pointInView))
  }

end
--------------

If we run the application we should see the following when we touch the screen:

[source, sh]
------------
> Touch 1 starting on {147, 305}
> Touch 2 starting on {117, 205}
> Touch 1 moving to {151, 297}
> Touch 2 moving to {179, 327}
> Touch 1 moving to {153, 294}
> Touch 2 moving to {178, 323}
> Touch 1 moving to {155, 289}
> Touch 2 moving to {179, 326}
> Touch 1 ended at {178, 319}
> Touch 2 ended at {198, 119}
------------

IMPORTANT: Are you using the iOS Simulator? No problem! You can also simulate multiple touches pressing **alt** and if you need to move the multiple touches together press **alt + shift** 


Painting the Touches
~~~~~~~~~~~~~~~~~~~~
Now that we have detected the touches, its time to paint them on the screen. For that we will use a technology called CoreGraphics, but before we use it we need to understand some key concepts:

* If we want to draw on a UIView using CoreGraphics we need to do it in a method called **drawRect**, its a override and you shouldn't implemented unless your will draw something (It affects performance when its not drawing)

* The drawing on a UIView using Core Graphics its not additive, what this means is that I can't draw a line and then later when the user touches the screen draw another (I can't  call **drawRect** manually). For that purpose we need to store which lines will be painted and then call **setNeedsDisplay**, this method will clear the entire view and then call **drawRect** again, forcing repainting everything again.

Using the above information lets implement the user touches on the screen:

["source","ruby", args="-O \"hl_lines=46 49 50\""]
--------------
# Method where we need to do the Core Graphics drawing
def drawRect(rect)

  # Get the Core Graphics current context
  context = UIGraphicsGetCurrentContext()
  
  # Set a color for drawing the touch points
  UIColor.colorWithRed(0.988, green:0.612, blue:0.157, alpha:1.0).set

  # Iterate the touch points
  @touch_points.each { | touch_point |

    # Move the context to the touch point
    CGContextMoveToPoint(context, 
                         touch_point.CGPointValue.x, 
                         touch_point.CGPointValue.y)
    
    # Create a rect in which want to the ellipse be drawn
    point_rect = CGRectMake(point.CGPointValue.x - 10, 
                            point.CGPointValue.y - 10,
                            20,
                            20)

    # Add the ellipse using the rect into the context
    CGContextAddEllipseInRect(context, point_rect)
  
    # Draw the context into the view
    CGContextFillPath(context)
  }
end

# This method is called when the finger (or fingers) 
# touch the screen for the first time
def touchesBegan(touches, withEvent: event)

  #  Touches is an set of UITouch, each of them
  #  represent a diferent finger on the screen
  touches.allObjects.each_with_index { | touch, index |

    # We need to ask the touch for his location according
    # to the current view
    pointInView = touch.locationInView(self)

    # Add the point to our array, but because is a structure (CGPoint)
    # we need to store it on a NSValue
    @touch_points.addObject(NSValue.valueWithCGPoint(pointInView))
  }

  # Ask the view to redraw again
  self.setNeedsDisplay

end
--------------

.Initial Touches
image::resources/ch19-Multitouch/ch19_InitialTouches.png[Initial Touches]

Great! Now we are really seeing where the user touches began, lets continue adding the touch movements:

IMPORTANT: If you are using a simulator probably you will notice that the dot is painted down right of the cursor. This is normal because iOS handle the mouse click as the top left corner of a finger

["source","ruby", args="-O \"hl_lines=13 16\""]
--------------
# This method is called when the finger (or fingers) 
# are moving without leaving the screen
def touchesMoved(touches, withEvent: event)

  #  Touches is an set of UITouch, each of them
  #  represent a diferent finger on the screen
  touches.allObjects.each_with_index { | touch, index |

    # We need to ask the touch for his location according
    # to the current view
    pointInView = touch.locationInView(self)

    @touch_points.addObject(NSValue.valueWithCGPoint(pointInView))  
  }

  self.setNeedsDisplay
end
--------------

.Initial Lines
image::resources/ch19-Multitouch/ch19_InitialLines.png[Initial Lines]

Now we are drawing the initial touches and the movement in the screen, it looks like a line but if we move the finger faster will become more like a dotted line right? No problem we will fix that later


Kanji Drawing
~~~~~~~~~~~~~
We just play a bit with the touches and drawing for us to feel comfortable about how it works, lets do something more challenging:

If we look on our background picture, we have a Kanji with traces marked with numbers. The objective is create a little game for the user to draw the Kanji, so our first task is to determinate if the user is following the traces or not

.Starting Points
image::resources/ch19-Multitouch/ch19_StartingPoints.png[Starting Points]


The first thing we can do is to have an array of the trace starting coordinates, that will enable us to evaluate if the touch was on the beginning. Lets do a little experiment to test this part:

["source","ruby", args="-O \"hl_lines=13 45 46 47 48 49 50 51 52 53 54 55 56 59 60 61 62 63 64 65 66 67 68\""]
--------------
def initWithFrame(frame)

  if super

    # In order to does not hide the background image on
    # the controllers view, we need to set the background color on
    # this view as transparent
    self.backgroundColor = UIColor.clearColor

    # Instance an Array for storing the user touches
    @touch_points = NSMutableArray.alloc.init

    load_kanji_traces
  end

  self

end

def load_kanji_traces

  # Create an array to store our Kanji Paths
  @kanji_traces = NSMutableArray.alloc.init

  # As a experiment lets add only the starting point of 
  # the trace number three
  kanji_starting_trace_three = CGPointMake(150, 260)


  # Add the point to the Kanji Paths array
  @kanji_traces.addObject(NSValue.valueWithCGPoint(kanji_starting_trace_three))
end



# This method is called when the finger (or fingers) 
# touch the screen for the first time
def touchesBegan(touches, withEvent: event)

  # We need to ask the touch for his location according
  # to the current view
  pointInView = touches.anyObject.locationInView(self)


  touch_at_beginning = false

  # Iterate through all the Kanji Paths available
  @kanji_traces.each { | kanji_trace |

    # If the touched point is equal to any Kanji Trace starting
    # point
    if CGPointEqualToPoint(kanji_trace.CGPointValue, pointInView)
      
      touch_at_beginning = true
    end
  }


  # If the touch was at the beginning of any Kanji Trace
  unless touch_at_beginning

    # Add the point to our array, but because is a structure (CGPoint)
    # we need to store it on a NSValue
    @touch_points.addObject(NSValue.valueWithCGPoint(pointInView))

    # Ask the view to redraw again
    self.setNeedsDisplay
  end

end
--------------

Impossible do it with the first touch (Without moving the touch), right? This is because we are comparing a small point in the screen with a wide of a finger, making it difficult to touch with precision. The solution to this is creating a rectangle surrounding the starting trace point, so it will be more easy to hit it with the finger.

Actually also we need to evaluate if the user is following the trace, and if it the trace ended where it should, for this we will use rectangles as well. In the light of this is better to create a object for all this variables.

IMPORTANT: Remember it does not matter if it is a point, rectangle or a button, it needs to be bigger enough for the user to touch

Lets create our new class **KanjiTrace**:

[source, sh]
------------
$ cd ..

$ cd mkdir models

$ cd models

$ touch kanji_trace.rb

$ open kanji_trace.rb
------------


[source, ruby]
--------------
class KanjiTrace 

  SURROUND_SIZE = 40

  attr_accessor :starting_point
  attr_accessor :end_point


  def initial_rectangle

    GPointMake(starting_point.x - SURROUND_SIZE,
               starting_point.y - SURROUND_SIZE,
               SURROUND_SIZE * 2,
               SURROUND_SIZE * 2)
  end


  def final_rectangle

    GPointMake(end_point.x - SURROUND_SIZE,
               end_point.y - SURROUND_SIZE,
               SURROUND_SIZE * 2,
               SURROUND_SIZE * 2)

  end

end
--------------

Great! Now we have a object containing the trace start and end points and it return the surrounding rectangles. Lets implement the **KanjiTrace** class into our **kanji_view.rb**:

[source, sh]
------------
$ cd ..

$ cd controllers

$ open kanji_view.rb
------------
 
["source","ruby", args="-O \"hl_lines=7 8 9 10 11 12 13 14 37 38 39 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60\""]
--------------
def load_kanji_traces

  # Create an array to store our Kanji Paths
  @kanji_traces = NSMutableArray.alloc.init


  # Continuing the experiment create only a Kanji Trace
  # for the trace number three
  trace_three = KanjiTrace.new

  trace_three.starting_point = CGPointMake(150, 260)

  # Add the point to the Kanji Paths array
  @kanji_traces.addObject(trace_three)
end



# This method is called when the finger (or fingers) 
# touch the screen for the first time
def touchesBegan(touches, withEvent: event)

  # We need to ask the touch for his location according
  # to the current view
  pointInView = touches.anyObject.locationInView(self)


  touch_at_beginning = false

  # Iterate through all the Kanji Paths available
  @kanji_traces.each { | kanji_trace |

    # If the touched point is equal to any Kanji Trace starting
    # point
    if CGRectContainsPoint(kanji_trace.initial_rectangle, pointInView)
   
      # Assign the trace that the user is currently
      # drawing
      @current_trace = kanji_trace
      touch_at_beginning = true
    end
  }


  # If the touch was at the beginning of any Kanji Trace
  if touch_at_beginning

    # Add the point to our array, but because is a structure (CGPoint)
    # we need to store it on a NSValue
    @touch_points.addObject(NSValue.valueWithCGPoint(pointInView))

    # Ask the view to redraw again
    self.setNeedsDisplay

  else

    # If there is no touch at the beginning means
    # that the user is not drawing a new trace
    @current_trace = nil
  end

end
--------------

.Start Trace #3
image::resources/ch19-Multitouch/ch19_StartTrace3.png[Start Trace #3]

Is working now! Is more easy for the user to touch the start of the trace, so lets continue adding the all the Kanji Traces:

[source, ruby]
--------------
def load_kanji_traces

  # Create an array to store our Kanji Paths
  @kanji_traces = NSMutableArray.alloc.init


  trace_one = KanjiTrace.new
  trace_one.starting_point = CGPointMake(40, 300)
  trace_one.end_point = CGPointMake(273, 282)

  @kanji_traces.addObject(trace_one)


  trace_two = KanjiTrace.new
  trace_two.starting_point = CGPointMake(155, 285)
  trace_two.end_point = CGPointMake(289, 366)

  @kanji_traces.addObject(trace_two)


  # Continuing the experiment create only a Kanji Trace
  # for the trace number three
  trace_three = KanjiTrace.new
  trace_three.starting_point = CGPointMake(150, 260)
  trace_three.end_point = CGPointMake(152, 367)
   
  # Add the point to the Kanji Paths array
  @kanji_traces.addObject(trace_three)


  trace_four = KanjiTrace.new
  trace_four.starting_point = CGPointMake(31, 366)
  trace_four.end_point = CGPointMake(145, 294)

  @kanji_traces.addObject(trace_four)


  trace_five = KanjiTrace.new
  trace_five.starting_point = CGPointMake(225, 175)
  trace_five.end_point = CGPointMake(226, 140)

  @kanji_traces.addObject(trace_five)


  trace_six = KanjiTrace.new
  trace_six.starting_point = CGPointMake(219, 202)
  trace_six.end_point = CGPointMake(277, 237)

  @kanji_traces.addObject(trace_six)


  trace_seven = KanjiTrace.new
  trace_seven.starting_point = CGPointMake(69, 191)
  trace_seven.end_point = CGPointMake(33, 160)

  @kanji_traces.addObject(trace_seven)


  trace_eight = KanjiTrace.new
  trace_eight.starting_point = CGPointMake(85, 211)
  trace_eight.end_point = CGPointMake(40, 247)

  @kanji_traces.addObject(trace_eight)


  trace_nine = KanjiTrace.new
  trace_nine.starting_point = CGPointMake(109, 171)
  trace_nine.end_point = CGPointMake(185, 159)

  @kanji_traces.addObject(trace_nine)

  
  trace_ten = KanjiTrace.new
  trace_ten.starting_point = CGPointMake(190, 171)
  trace_ten.end_point = CGPointMake(186, 241)

  @kanji_traces.addObject(trace_ten)


  trace_eleven = KanjiTrace.new
  trace_eleven.starting_point = CGPointMake(175, 227)
  trace_eleven.end_point = CGPointMake(114, 235)

  @kanji_traces.addObject(trace_eleven)


  trace_twelve = KanjiTrace.new
  trace_twelve.starting_point = CGPointMake(112, 223)
  trace_twelve.end_point = CGPointMake(112, 181)

  @kanji_traces.addObject(trace_twelve)


  trace_thirteen = KanjiTrace.new
  trace_thirteen.starting_point = CGPointMake(114, 161)
  trace_thirteen.end_point = CGPointMake(140, 129)

  @kanji_traces.addObject(trace_thirteen)

end
--------------

.All Starting Traces
image::resources/ch19-Multitouch/ch19_AllStartingTraces.png[All Starting Traces]

Yes! We can start painting on any trace now, so lets continue adding a method to our **KanjiTrace** for evaluate if the user is following the line and assigning a score for the drawing:

[source, sh]
------------
$ cd ..

$ cd models

$ open "kanji_trace.rb"
------------

[source, ruby]
--------------
attr_reader :accurate_points 

def evaluate_point(point)

  # Lets create properties to store the accurate touches of
  # the user and the missing ones
  @accurate_points ||= 0
  @missing_points ||= 0

  # The next step is creating a surrounding rectangle for the
  # trace, make it more easy for the user to touch
  trace_rect = CGRectMake([starting_point.x, end_point.x].min - SURROUND_SIZE, 
                          [starting_point.y, end_point.y].min - SURROUND_SIZE, 
                          (starting_point.x - end_point.x).abs + SURROUND_SIZE * 2, 
                          (starting_point.y - end_point.y).abs + SURROUND_SIZE * 2)


  # If the point is in the trace
  if CGRectContainsPoint(trace_rect, point)

    # Add one to the accurate points
    @accurate_points += 1
  else

    # Add one to the missing points
    @missing_points += 1
  end

end
--------------

Our **KanjiTrace** is now evaluating and recording the score, next step is to used in our **kanji_view.rb** on the touchesMoved method:

[source, sh]
------------
$ cd ..

$ cd views

$ open kanji_view.rb
------------


["source","ruby", args="-O \"hl_lines=10 11 12 13 14 15 16 17 18 19 20 21 22 23\""]
--------------
# This method is called when the finger (or fingers) 
# are moving without leaving the screen
def touchesMoved(touches, withEvent: event)

  # We need to ask the touch for his location according
  # to the current view
  pointInView = touches.anyObject.locationInView(self)


  # If the user is drawing a trace
  unless @current_trace.nil?

    # Add the touch point into the array, so it can be
    # drawing later
    @touch_points.addObject(NSValue.valueWithCGPoint(pointInView))

    # Evaluate how accurate is the touch in relationship
    # to the trace
    @current_trace.evaluate_point(pointInView)

    # Ask for repainting
    self.setNeedsDisplay
  end

end
--------------

Yeah now we are keeping a score about how accurate is the user drawing, but we are not doing nothing with it! Our next step is just about that!

.Painted Trace
image::resources/ch19-Multitouch/ch19_PaintedTrace.png[Painted Trace]

High Score!
~~~~~~~~~~~
So as the Kanji description tell us is that is a technique for building cups of tea, so what about adding some disable cups on the top of the app to symbolize the score of the user (3 disable cups mean 0 right?)

So the first thing will be copy the images in **DEFINE DEPLOYMENT PATH** into our app resource's folder, next add the follow to the **kanji_view.rb**:

["source","ruby", args="-O \"hl_lines=13\""]
--------------
def initWithFrame(frame)

  if super

    # In order to does not hide the background image on
    # the controllers view, we need to set the background color on
    # this view as transparent
    self.backgroundColor = UIColor.clearColor

    # Instance an Array for storing the user touches
    @touch_points = NSMutableArray.alloc.init

    layout_cups
    load_kanji_traces

  end

  self

end


def layout_cups

  first_cup_frame = CGRectMake(55, 29, 50, 50)

  @first_cup_image_view = UIImageView.alloc.initWithFrame(first_cup_frame)

  @first_cup_image_view.image = UIImage.imageNamed('bgRakuOff1.png')

  self.addSubview(@first_cup_image_view)


  second_cup_frame = CGRectMake(135, 29, 50, 50)

  @second_cup_image_view = UIImageView.alloc.initWithFrame(second_cup_frame)

  @second_cup_image_view.image = UIImage.imageNamed('bgRakuOff2.png')

  self.addSubview(@second_cup_image_view)


  thrid_cup_frame = CGRectMake(215, 29, 50, 50)

  @thrid_cup_image_view = UIImageView.alloc.initWithFrame(thrid_cup_frame)

  @thrid_cup_image_view.image = UIImage.imageNamed('bgRakuOff3.png')

  self.addSubview(@thrid_cup_image_view)

end
--------------

.Score Board
image::resources/ch19-Multitouch/ch19_ScoreBoard.png[Score Board]

The last part of the exercise consist of analyzing the score of all the traces and according to that change the images of the cups. Making a little scoreboard!

["source","ruby", args="-O \"hl_lines=4\""]
--------------
# Method where we need to do the Core Graphics drawing
def drawRect(rect)

  update_score_board 

  # Get the Core Graphics current context
  context = UIGraphicsGetCurrentContext()

  # Set a color for drawing the touch points
  UIColor.colorWithRed(0.988, green:0.612, blue:0.157, alpha:0.6).set

  # Iterate the touch points
  @touch_points.each { | touch_point |

    # Move the context to the touch point
    CGContextMoveToPoint(context, 
                         touch_point.CGPointValue.x, 
                         touch_point.CGPointValue.y)

  # Create a rect in which want to the ellipse be drawen
  point_rect = CGRectMake(touch_point.CGPointValue.x - 10, 
                          touch_point.CGPointValue.y - 10,
                          20,
                          20)

  # Add the ellipse using the rect into the context
  CGContextAddEllipseInRect(context, point_rect)

  # Draw the context into the view
  CGContextFillPath(context)
  }
end


def update_score_board

  total_accurate_points = 0

  @kanji_traces.each { | kanji_trace |

      unless kanji_trace.accurate_points.nil?

        total_accurate_points += kanji_trace.accurate_points
      end      
  }

  if total_accurate_points >= 100

    @first_cup_image_view.image = UIImage.imageNamed('bgRakuOn1.png')
  end

  if total_accurate_points >= 300

    @second_cup_image_view.image = UIImage.imageNamed('bgRakuOn2.png')
  end

  if total_accurate_points >= 500

    @thrid_cup_image_view.image = UIImage.imageNamed('bgRakuOn3.png')
  end
end
--------------

.High Score
image::resources/ch19-Multitouch/ch19_HighScore.png[High Score]

Now is finished! It looks like it will need some animations for the cups to look amazing, in the next chapters we will see how to make animations

Challenges
~~~~~~~~~~
. Restart the game when the user double tap the screen (Hint: tapCount)

. Implement a point subtraction of the trace score, when the user draws outside the trace, but without loosing the ability to  get the maximum score

. The current implementation is drawing every point into the view, the challenge is change that into drawing just one line starting in the first touch and ending when the user removes the finger (Hint: CGContextAddLineToPoint)

